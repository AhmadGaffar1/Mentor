"""
FIND VIDEOs METADATA USING ( YOUTUBE API: all metadata) , ( ASSEMPLYAI: transcript )

Main entry point:
    process_videos(id: UUID, url: str) -> dict[str, object]

Features:
- Validate and extract YouTube video id
- Get metadata via YouTube Data API (cached)
- Try native YouTube captions first (if available)
- Otherwise: download audio via pytube -> upload to AssemblyAI -> poll transcript
- Exponential backoff for rate limits and transient errors
- Careful logging and cleanup of temp files
"""

from __future__ import annotations
import os
import re
import time
import json
import random
import logging
from pathlib import Path
from uuid import UUID
from functools import lru_cache

import requests
from pytube import YouTube, exceptions as pytube_exceptions
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from urllib.error import HTTPError, URLError
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
from APP.Configration import YOUTUBE_API_KEY, ASSEMBLYAI_API_KEY, MAX_TIME_FOR_TRANSCRIPT_EXTRACTION, GLOBAL_REQUEST_DELAY


# -------------------------
# Logging (module-level)
# -------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)

LOG = logging.getLogger(__name__)

# -------------------------
# Constants
# -------------------------
YOUTUBE = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
ASSEMBLY_HEADERS = {"authorization": ASSEMBLYAI_API_KEY}
UPLOAD_ENDPOINT = "https://api.assemblyai.com/v2/upload"
TRANSCRIPT_ENDPOINT = "https://api.assemblyai.com/v2/transcript"

# -----------------schema--------------------------------------------------
# DEFAULT_META: Default result schema
# -------------------------------------------------------------------
# Represents the unified metadata schema for any processed YouTube video.
# This structure ensures consistent keys regardless of whether data comes
# from the YouTube Data API or AssemblyAI transcription.
#
# - `video_id` is mandatory and used to fetch all other metadata fields.
# - All other fields are optional and may remain None if unavailable.
#
# Field definitions:
# -------------------------------------------------------------------
DEFAULT_META: dict[str, object] = {
    # Basic metadata (fetched via YouTube Data API)
    "video_id": None,       # Unique YouTube video identifier (always required)
    "title": None,          # Video title
    "link": None,           # Video link
    "description": None,    # Description that existed under YouTube Video that known by (Description Box)
    "channel": None,        # Channel or author name
    "duration": None,       # ISO 8601 duration string (e.g., "PT10M15S")
    "has_captions": None,   # Boolean indicating if YouTube captions exist

    # (Captions Text: Provided by YouTube Captions API) or (Transcript Text: Transcribed by AssemblyAI API as external tool)
    "text": None,

    # Transcript source
    # Specifies which system generated `text`:
    #   - "YouTube Captions" → fetched via YouTube Captions API
    #   - "AssemblyAI"       → generated by AssemblyAI speech-to-text
    "transcript_source": None,

    # Extended data (populated only if AssemblyAI was used)
    "summary": None,        # AI-generated summary of the transcript
    "chapters": [],         # List of chapters with start/end times + headlines
}
# -------------------------------------------------------------------

# -------------------------
# Utility helpers
# -------------------------
YOUTUBE_ID_RE = re.compile(r"(?:v=|youtu\.be/|embed/)([A-Za-z0-9_-]{11})")


def extract_video_id(url: str) -> Optional[str]:
    """Extract 11-char YouTube video id from common URL formats."""
    if not isinstance(url, str):
        return None
    m = YOUTUBE_ID_RE.search(url)
    if m:
        return m.group(1)
    # as fallback, accept raw 11-char id
    if re.fullmatch(r"[A-Za-z0-9_-]{11}", url):
        return url
    return None


def is_valid_youtube_id(video_id: str) -> bool:
    return bool(re.fullmatch(r"[A-Za-z0-9_-]{11}", video_id))


def jittered_sleep(base: float):
    """Sleep with small random jitter to avoid strict patterns."""
    time.sleep(base + random.uniform(0, base * 0.25))


# -------------------------
# YouTube metadata (cached)
# -------------------------
# Cache works automatically — but only if:
#   - The function is pure (no side effects, depends only on inputs).
#   - Arguments are hashable (e.g., strings, numbers, tuples — not lists or dicts).
# If you ever want to clear the cache:
#   - get_video_info.cache_clear()
# Or check the stats of cache:
#   - print(get_video_info.cache_info())
# Example for cache status:
#   - CacheInfo(hits=5, misses=2, maxsize=100, currsize=2)
# Use cache only by below line:
# -------------------------
@lru_cache(maxsize=256)
def get_video_info(video_id: str, url: str) -> Optional[Dict[str, Any]]:
    try:
        resp = YOUTUBE.videos().list(part="snippet,contentDetails", id=video_id).execute()
        items = resp.get("items", [])
        if not items:
            return None
        video = items[0]
        snippet = video.get("snippet", {})
        info = {
            "video_id": video_id,
            "title": snippet.get("title"),
            "link": url,
            "description": snippet.get("description"),
            "channel": snippet.get("channelTitle"),
            "duration": video.get("contentDetails", {}).get("duration"),
            "has_captions": False,
        }
        try:
            caps = YOUTUBE.captions().list(part="snippet", videoId=video_id).execute()
            info["has_captions"] = len(caps.get("items", [])) > 0
        except HttpError as e:
            # quota errors or permission; keep has_captions as False
            LOG.warning(f"[YouTube API] captions.list error for {video_id}: {e}")
        return info
    except HttpError as e:
        LOG.error(f"[YouTube API HTTP Error] {e}")
        return None
    except Exception as e:
        LOG.error(f"[YouTube] Failed to fetch metadata for {video_id}: {e}")
        return None


# -------------------------
# YouTube captions fallback
# -------------------------
def fetch_youtube_captions(video_id: str, prefer_langs: list[str] = ["en"]) -> Optional[str]:
    """Robust attempt to fetch YouTube captions if present."""
    try:
        transcripts = YouTubeTranscriptApi.list_transcripts(video_id)

        # prefer manual, then generated, then any that match preferred prefix
        for mode in ("manually_created", "generated"):
            for lang in prefer_langs:
                try:
                    if mode == "manually_created":
                        t = transcripts.find_manually_created_transcript([lang])
                    else:
                        t = transcripts.find_generated_transcript([lang])
                    items = t.fetch()
                    return " ".join(item["text"] for item in items)
                except NoTranscriptFound:
                    continue
                except Exception as e:
                    LOG.debug(f"[Captions] fetch error ({mode}, {lang}) for {video_id}: {e}")

        # try any transcript with matching language prefix
        for t in transcripts:
            lc = getattr(t, "language_code", "")
            if any(lc.startswith(pl) for pl in prefer_langs):
                try:
                    items = t.fetch()
                    return " ".join(x["text"] for x in items)
                except Exception:
                    continue

        # try translation if available
        for t in transcripts:
            try:
                trans = t.translate(prefer_langs[0])
                items = trans.fetch()
                return " ".join(x["text"] for x in items)
            except Exception:
                continue

        LOG.info(f"[Captions] No captions found for {video_id}")
        return None

    except TranscriptsDisabled:
        LOG.info(f"[Captions] Transcripts disabled by uploader for {video_id}")
        return None
    except NoTranscriptFound:
        LOG.info(f"[Captions] No transcripts found for {video_id}")
        return None
    except Exception as e:
        # Could be rate-limited or other
        LOG.warning(f"[Captions] Unexpected error fetching captions for {video_id}: {e}")
        return None


# -------------------------
# Download audio (pytube)
# -------------------------
class YouTubeDownloadError(Exception):
    pass


def download_audio(video_id: str, output_dir: str = "downloads", max_retries: int = 3) -> Path:
    if not is_valid_youtube_id(video_id):
        raise YouTubeDownloadError(f"Invalid YouTube video id: {video_id}")

    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    filename = f"{video_id}.mp3"
    out_path = out_dir / filename
    url = f"https://www.youtube.com/watch?v={video_id}"

    attempt = 0
    while attempt < max_retries:
        try:
            LOG.info(f"[YouTube] Download attempt {attempt+1} for {video_id}")
            yt = YouTube(url)
            stream = yt.streams.filter(only_audio=True).first()
            if stream is None:
                raise YouTubeDownloadError("No audio stream available")
            saved = stream.download(output_path=str(out_dir), filename=filename)
            LOG.info(f"[YouTube] Downloaded audio to {saved}")
            return Path(saved)
        except pytube_exceptions.VideoUnavailable:
            raise YouTubeDownloadError(f"Video unavailable or restricted: {video_id}")
        except pytube_exceptions.RegexMatchError:
            raise YouTubeDownloadError(f"Invalid video URL or ID: {video_id}")
        except Exception as e:
            LOG.warning(f"[YouTube] transient download error for {video_id}: {e}")
            attempt += 1
            jittered_sleep(2 ** attempt)  # increasing wait
    raise YouTubeDownloadError(f"Failed to download audio after {max_retries} attempts for {video_id}")


# -------------------------
# AssemblyAI integration
# -------------------------
class AssemblyAIError(Exception):
    pass


def upload_to_assemblyai(file_path: Path, timeout: int = 60) -> str:
    """Upload local audio file to AssemblyAI and return the upload_url."""
    if not file_path.exists():
        raise AssemblyAIError(f"File not found: {file_path}")
    LOG.info(f"[AssemblyAI] Uploading {file_path.name}")
    with file_path.open("rb") as fh:
        try:
            resp = requests.post(UPLOAD_ENDPOINT, headers=ASSEMBLY_HEADERS, data=fh, timeout=timeout)
            resp.raise_for_status()
            upload_url = resp.json().get("upload_url")
            if not upload_url:
                raise AssemblyAIError(f"No upload_url returned: {resp.text}")
            return upload_url
        except requests.HTTPError as e:
            raise AssemblyAIError(f"Upload HTTP error: {e} - {getattr(e.response, 'text', '')}") from e
        except Exception as e:
            raise AssemblyAIError(f"Upload failed: {e}") from e


def start_transcription(upload_url: str, enable_speakers: bool = True, enable_chapters: bool = True) -> str:
    payload = {
        "audio_url": upload_url,
        "speaker_labels": enable_speakers,
        "auto_chapters": enable_chapters,
        "summary_model": "informative",
    }
    try:
        resp = requests.post(TRANSCRIPT_ENDPOINT, json=payload, headers=ASSEMBLY_HEADERS, timeout=30)
        resp.raise_for_status()
        job_id = resp.json().get("id")
        if not job_id:
            raise AssemblyAIError(f"No job id returned: {resp.text}")
        LOG.info(f"[AssemblyAI] Job started: {job_id}")
        return job_id
    except requests.HTTPError as e:
        raise AssemblyAIError(f"Start transcription HTTP error: {e} - {getattr(e.response, 'text', '')}") from e
    except Exception as e:
        raise AssemblyAIError(f"Start transcription failed: {e}") from e


def poll_transcription(job_id: str, max_wait: int = MAX_TIME_FOR_TRANSCRIPT_EXTRACTION, poll_interval: int = 5) -> Dict[str, Any]:
    """Poll AssemblyAI until done or timeout. Returns the transcript JSON (text, summary, chapters)."""
    start = time.time()
    endpoint = f"{TRANSCRIPT_ENDPOINT}/{job_id}"
    attempt = 0

    while True:
        try:
            resp = requests.get(endpoint, headers=ASSEMBLY_HEADERS, timeout=30)
            resp.raise_for_status()
            data = resp.json()
        except requests.HTTPError as e:
            # on 429 or 5xx, backoff and retry
            status = getattr(e.response, "status_code", None)
            LOG.warning(f"[AssemblyAI] polling HTTPError status={status} job={job_id}: {e}")
            if status == 429 or (status and 500 <= status < 600):
                wait = min(10 * (2 ** attempt), 300) + random.uniform(0, 3)
                LOG.warning(f"[AssemblyAI] rate/server-limited; backing off {wait:.1f}s (attempt {attempt+1})")
                time.sleep(wait)
                attempt += 1
                continue
            raise AssemblyAIError(f"Polling HTTP error: {e}") from e
        except Exception as e:
            LOG.warning(f"[AssemblyAI] transient polling error: {e}")
            time.sleep(5)
            continue

        status = data.get("status")
        if status == "completed":
            LOG.info(f"[AssemblyAI] job {job_id} completed")
            return {
                "text": data.get("text", "") or "",
                "summary": data.get("summary", None),
                "chapters": data.get("chapters", []) or [],
            }
        if status == "error":
            raise AssemblyAIError(f"Transcription job failed: {data.get('error')}")
        if time.time() - start > max_wait:
            raise AssemblyAIError("Transcription polling timed out")
        # polite poll interval + slight jitter
        time.sleep(poll_interval + random.uniform(0, 1))


# -------------------------
# Main process function
# -------------------------
def process_videos(id: UUID, url: str) -> Dict[str, object]:
    """
    Process a single YouTube URL:
      - extract id, fetch metadata
      - try youtube captions first (if available)
      - otherwise download audio and transcribe with AssemblyAI
    """

    start_time = time.time()
    result = dict(DEFAULT_META)
    result["link"] = url

    video_id = extract_video_id(url)
    if not video_id:
        LOG.error(f"Invalid YouTube URL: {url}")
        result["error"] = "YouTube Error 400: Invalid or non-existent video ID"
        return result

    result.update(get_video_info(video_id, url) or dict(DEFAULT_META))
    LOG.info(f"Processing video: {result.get('title') or video_id}")
    result["video_id"] = video_id

    # Try YouTube captions first (cheap, no service usage)
    try:
        if result.get("has_captions"):
            caps_text = fetch_youtube_captions(video_id)
            if caps_text:
                result["text"] = caps_text
                result["transcript_source"] = "YouTube Captions"
                elapsed = time.time() - start_time
                LOG.info(f"Finished (captions) {video_id} in {elapsed:.2f}s")
                return result
    except Exception as e:
        LOG.warning(f"[Captions] fallback error for {video_id}: {e}")

    # Otherwise use AssemblyAI
    audio_path: Optional[Path] = None
    try:
        # Download audio (retries inside)
        audio_path = download_audio(video_id, output_dir="downloads", max_retries=3)

        # Wait a little before uploading (avoid pattern detection)
        jittered_sleep(3)

        # Upload + start transcription
        upload_url = upload_to_assemblyai(audio_path)
        job_id = start_transcription(upload_url)

        # Poll and collect results (with timeout)
        transcription = poll_transcription(job_id, max_wait=MAX_TIME_FOR_TRANSCRIPT_EXTRACTION, poll_interval=5)

        # Populate result
        result["text"] = transcription.get("text", "")
        result["summary"] = transcription.get("summary")
        result["chapters"] = transcription.get("chapters", [])
        result["transcript_source"] = "AssemblyAI"
        LOG.info(f"[Done] transcription complete for {video_id}")

    except Exception as e:
        LOG.error(f"error failed for {video_id}: {e}")
        result["text"] = result.get("text") or None
        result["summary"] = result.get("summary") or None
        result["chapters"] = result.get("chapters") or []
        result["error"] = str(e)
    finally:
        # Clean up local audio file if present
        try:
            if audio_path and audio_path.exists():
                try:
                    audio_path.unlink()
                    LOG.debug(f"Deleted temp audio {audio_path}")
                except Exception as ex:
                    LOG.debug(f"Could not delete temp audio: {ex}")
        except Exception:
            pass

    elapsed = time.time() - start_time
    LOG.info(f"Finished processing {video_id} in {elapsed:.2f} seconds")
    return result